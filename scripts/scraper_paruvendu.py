import requests
from bs4 import BeautifulSoup
import time
import random
import csv
import os

# Configuration des chemins et URL
OUTPUT_PATH = "backend/data/annonces_lyon_paruvendu.csv"
base_url = "https://www.paruvendu.fr/immobilier/recherche/location/lyon/?rechpv=1&tt=5&tbApp=1&tbDup=1&tbChb=1&tbLof=1&tbAtl=1&tbPla=1&tbMai=1&tbVil=1&tbCha=1&tbPro=1&tbHot=1&tbMou=1&tbFer=1&nbp0=99&pa=FR&lol=0&ray=50&codeINSEE=69000,"

# Headers pour imiter un navigateur et √©viter le blocage
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

if __name__ == '__main__':
    print("ü¶Å Lancement du Scraper ParuVendu (Mode Rapide)...")
    
    # Cr√©ation du dossier de destination s'il n'existe pas
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    
    liens_vus = set()
    page_num = 1
    continuer = True

    with open(OUTPUT_PATH, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Titre', 'Prix', 'Lien'])

        while continuer:
            # Gestion de l'URL pour la pagination
            url_page = base_url if page_num == 1 else f"{base_url}&p={page_num}"
            
            print(f"\n--- üìÑ Analyse de la Page {page_num} ---")
            
            try:
                # On utilise r (alias de requests) comme tu l'as sp√©cifi√© dans tes pr√©f√©rences
                import requests as r
                response = r.get(url_page, headers=headers, timeout=10)
                
                if response.status_code != 200:
                    print(f"üõë Erreur de r√©ponse : {response.status_code}")
                    break
                    
            except Exception as e:
                print(f"‚ùå Erreur connexion : {e}")
                break

            soup = BeautifulSoup(response.text, "html.parser")
            annonces = soup.find_all("article", class_="blocAnnonce")
            
            if not annonces:
                print("‚ùå Aucune annonce trouv√©e (fin des r√©sultats).")
                break

            compteur_page = 0
            for annonce in annonces:
                try:
                    # R√©cup√©ration des √©l√©ments
                    titre_element = annonce.find("a", class_="popinphoto_liste_titre")
                    prix_element = annonce.find("div", class_="popinphoto_liste_prix")

                    if titre_element and prix_element:
                        # Nettoyage des donn√©es
                        titre = " ".join(titre_element.text.split())
                        prix = prix_element.text.strip()
                        
                        # R√©cup√©ration du lien
                        lien_partiel = titre_element.get('href')
                        lien_complet = f"https://www.paruvendu.fr{lien_partiel}" if lien_partiel else "Pas de lien"
                        
                        # V√©rification de l'unicit√©
                        if lien_complet in liens_vus or lien_complet == "Pas de lien":
                            continue
                        
                        # √âcriture
                        writer.writerow([titre, prix, lien_complet])
                        liens_vus.add(lien_complet)
                        compteur_page += 1
                        print(f"üè† {titre} -- üí∞ {prix}")
                        
                except Exception:
                    continue
            
            print(f"‚úÖ Page {page_num} termin√©e : {compteur_page} annonces ajout√©es.")

            # Condition d'arr√™t : si on n'a plus de nouvelles annonces sur cette page
            if compteur_page == 0:
                print("üèÅ Plus de nouvelles annonces disponibles.")
                continuer = False
            else:
                page_num += 1
                # Petite pause pour √™tre poli avec le serveur
                time.sleep(random.uniform(1.5, 3))

    print(f"\n‚ú® Termin√© ! Total : {len(liens_vus)} annonces sauvegard√©es dans {OUTPUT_PATH}")